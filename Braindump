# Braindump

In the Quarto presentation, do a comparison of the data cleaning methods across the two approaches. State that they are quite different indeed and present some statistics on how they differ? The differences in data cleaning methods made the results uncomparable. Now, let me present the results as classified by Kehoe's SSVM and Dorde's classifiers on the same dataset, as cleaned by Dorde. and  Present SSVM from Kehoe along with Dordes methods.

Get classification accuracy from Dorde's classifiers and Kehoe's SSVM

Get file.R names from Dorde's thesis code snippets

Targeted features are done by Skyline which is fed a combined feature set; thus, fe

Iterative feature removal removes features between healthy control groups so that they have the same features?

What stage is the dataset in which Dorde is referring to? Pretty raw, methinks.

How to have a system explaining, in detail, what the code is doing?

Make the Quarto presentation so that you have specific things in the code that you point out as important.

Test running Dorde's R code in Quarto.
Explain the classifiers in the presentation

Only the one Python script should really be included in the Quatro-document, that is


How do I get the exact same samples as Dorde? Also ask Dorde about the packages.

Combining feature sets?

XCMS

Will I need iterative_feature_removal at all?

- presentation: simply run the pipeline in Quarto!

- https://programmathically.com/

- borsta mellan wc kakel med tandborste!

- is it soft margin or hard marginn ssvm? Check the code.
can't access the information since I don't have the calcom library.

- how is Dordes pipeline automated? Because of the hyperparameter tuning done as informed by ROC?

- calcom vs calcom.io?

- is ROC or AUROC mentioned in Kehoe?

- pip install in conda environment? Remember, different packages are available in pip and conda.

- why is data science so interesting?

- the code was indeed in Dordes thesis! Always check the code for answers which Leo may or may not have!

- is variable hyperparameter tuning input usually recorded somewhere for reproducing the results?

-train-validation split same as in Dorda?

- Dordes email or contact on linked in

Are the classifiers that Dorde used detailed in the publication? Or do you need access to the source code?

You might be able to tease apart the hyperparameter tuning steps from the publication!

Classifiers used from publication text directly?
Can I get the R code from Djordjes work directly?
Targeted features vs features

Reproducibility vs replicability:
Reproducibility is the extent to which the exact workflow of a study can be followed to produce identical results. Replicability, on the other hand, focuses on whether a hypothesis can be confirmed across different experimental setups and datasets.

must your libraries be in the same folder?(this was the case with ikkunasto)
- I don't think so, this is only the case for

- write to Leo asking about splitting validation data from training data/cross validation, but first check how it is done in the code

Use the date to access the correct versions of packages (can they be accessed that way?)? Is this a reasonable assumption or should I ask for the exact versions?

Virtual environments in research?

Variable user input in replication of study?

The scripts may well work without using exact same versions as in the publication but how would I know whether it's the variable user input or the scripts which cause results differing from those in the publication?

Will I be using virtual environments in my workflow?

**This is extra storage for short term memory and mini-hypotheses, which is very dynamic in that much of it is deleted or brought to bear at some other location. Yet, it is probably useful to formulate the brainstorm into complete sentences, as this is often enough to have clarity in the matter at hand**

Use the glioma modeling code as help!?

- exactly what calculations does the calcom library perform? Feature engineering?

-can you just install the script directly by pip or conda? let's see!
