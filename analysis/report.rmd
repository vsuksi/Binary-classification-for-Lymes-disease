---
title: "Lyme classification report"
author: "Đorđe Klisura, Chouaib Benchraka, Vilhelm Suksi Leo Lahti"
date: 'r Date()'
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mikropml)
library(data.table)
library(umap)
library(ggfortify)
library(VIM)
library(mt)
library(cluster)
library(kableExtra)
library(rmdformats)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
#| label: load-packages
#| echo: false

#library(ggplot2)
knitr::opts_chunk$set(dev = "png",
                      dev.args = list(type = "cairo-png"),
		      warning=FALSE,
		      message=FALSE,
		      echo=FALSE
		      )

library(scater)
library(mia)
library(dplyr)
# Load packages
library(TreeSummarizedExperiment)

source("funcs.R")
set.seed(142)

# Read in the data (assuming standardized format!)
source("data.R")

# Split the full data into training and test sets
# The modeling functions will never see the test set

## Training data
train_test_split = 0.8
train <- sample(ncol(tse), round(train_test_split*ncol(tse)))
tse.train <- tse[, train]

## Independent test data
test <- setdiff(seq(ncol(tse)), train)
tse.test  <- tse[, test]
```

Analysis of the following data set:
- `r dataset`.


# Sample similarity

The samples are log10-transformed and features scaled before ordination.

## Unsupervised ordination

Ordination is shown here for all samples combined.

```{r}
# Add log10 transformation and scaling for the signal assay
pseudocount <- 0
if (min(assay(tse, "signal"))==0) {
  x <- assay(tse, "signal")
  pseudocount <- min(x[x>0])/2
}
tse <- mia::transformSamples(tse, abund_values="signal", method="log10", pseudocount=pseudocount)
tse <- mia::transformFeatures(tse, abund_values="log10",  method="z")

# Run UMAP
tse <- scater::runUMAP(tse, name = "UMAP", exprs_values = "z")

# Plot UMAP
p.umap <- plotReducedDim(tse, "UMAP", colour_by = "Group") + labs(title = "UMAP")
```

```{r}
# Run PCA
tse <- scater::runPCA(tse, name = "PCA", exprs_values = "z")

# Plot PCA
p.pca <- plotReducedDim(tse, "PCA", colour_by = "Group") + labs(title = "PCA")
```

```{r, fig.width=10, fig.height=4}
# Show the plots
library(patchwork)
print(p.umap + p.pca)

# Define for next chunk
nclust <- 4
```

## Unsupervised clustering

With `r nclust` clusters.

```{r}
library(bluster)
cl <- clusterRows(t(assay(tse, "z")), HclustParam(k=nclust, method="ward.D2"))
# cl <- cluster::pam(t(assay(tse, "signal")), k=2)
# cl <- cluster::fanny(t(assay(tse, "z")), 2)
plotUMAP(tse, colour_by=I(cl))
```



# Classifier results

```{r pipeline, echo=FALSE}
# Set parameters
method <- "glmnet"
cv_times <- 2
training_frac = 0.98 # Set to this since 1 is not accepted; train with (almost) full training data
kfold <- 5
outcome_variable <- "Group"

#if(method=="SSVM")
#{
#  SSVM <- TRUE
#}else{
  library(mikropml)
  dataset <- as.data.frame(t(assay(tse.train, "signal")))
  dataset$Group=colData(tse.train)$Group
  res <- run_ml(dataset,
                method,
                outcome_colname = outcome_variable,
                kfold = kfold,
                cv_times = cv_times,
                training_frac = training_frac,
                seed = 2019)

                training_data <- res$trained_model$trainingData %>%
                  dplyr::rename(dx = .outcome)
                test_data <- res$test_data

                hyperparameters <- get_hyperparams_list(dataset, method)

                cross_val <- define_cv(training_data,
                                       "dx",
                                       hyperparameters,
                                       perf_metric_function = caret::multiClassSummary,
                                       class_probs = TRUE,
                                       cv_times = 2)

                                       tune_grid <- get_tuning_grid(hyperparameters, method)

                                       model_formula <- stats::as.formula(paste("dx", "~ ."))
                                             model <- caret::train(
                                               form = model_formula,
                                               data = training_data,
                                               method = method,
                                               metric = "AUC",
                                               trControl = cross_val,
                                               tuneGrid = tune_grid)


#}
```

The pipeline was run with the following parameters

- Method: `r method`
- CV folds: `r cv_times`
- Training fraction: `r train_test_split`
- Folds: `r kfold`



Predicted values and class probabilities for the independent test data

```{r}
test_data <- as.data.frame(t(assay(tse.test, "signal")))
test_data$Group=colData(tse.test)$Group
pred <- predict(model, newdata = test_data)
pred <- data.frame(
          Sample = colData(tse.test)$Sample,
          Original = colData(tse.test)$Group,
          Prediction=pred)

# Prediction probabilities
prob <- predict(model, newdata = test_data, type="prob")
pred <- cbind(pred, prob)

pred %>% kable(digits=2)
```



## Confusion matrix

This shows classification performance for the independent test samples.

```{r}
predicted <- as.factor(pred$Prediction)
original  <- as.factor(pred$Original)
confusionMatrix(predicted, original)
```

```{r}
feature_importance <- read.csv("feature_importance_03112022.csv")

perm_top <- feature_importance %>%
  group_by(names)%>%
  summarise(median = median(perf_metric_diff), iqr_AUC = IQR(perf_metric_diff), mean = mean(perf_metric_diff), se = sd(perf_metric_diff)/sqrt(n())) %>%
  mutate(sign = case_when(median > 0 ~ "positive", median < 0 ~ "negative")) %>%
  arrange(-median) %>%
  head(n=10) %>%
  select(names, median, iqr_AUC, mean, se)

#ggplot2 bar plot of top features
p.plot <- ggplot(perm_top, aes(reorder(names, mean), mean)) +
 geom_bar(position = position_dodge(), width = .25, stat="identity", fill="steelblue")  +
 geom_hline(yintercept = 0, color = "black") +
        geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width=0.2) +
 theme(panel.grid.major = element_blank(),
       panel.grid.minor = element_blank()) +
 theme_bw() +
 theme(panel.grid.major = element_blank(),
       panel.grid.minor = element_blank()) +
 xlab("Features") +
 ylab('Mean difference between test and permuted AUROC') +
 coord_flip() +
 theme(axis.text.x = element_text(size = 10,  colour=c("black")),
       axis.text.y = element_text(size = 10, colour=c("black")),
       axis.title.x = element_text(size=12, vjust = 0),
       axis.title.y = element_text(size=12, vjust = 0.5),
       legend.text = element_text(size=13))

p.plot
```

# Notes

Four distinct training methods are available: decision trees, random forest, SVM with radial basis kernel, and gradient boosted trees, as well as additional outputs such as cluster plot, probability ellipse plot, feature importance, and treating missing values with k-nearest neighbors. Furthermore, hyperparameters may be tuned separately and adjusted by user.
